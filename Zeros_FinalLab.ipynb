{"cells":[{"cell_type":"markdown","source":["# LSTM model"],"metadata":{"id":"F2u3wJbMg4ER"}},{"cell_type":"markdown","metadata":{"id":"Ao_5uzqajH04"},"source":["Import the necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90372,"status":"ok","timestamp":1655304350067,"user":{"displayName":"Klaus Ditterich Martin","userId":"12554408970924757965"},"user_tz":-120},"id":"4DVSD8xUb36E","outputId":"2cc02a8c-bf90-4d95-fb07-91245418f10f"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"," all_words.txt\t data_clean.csv\t\t 'IMDB Dataset.csv'\n"," data_1\t\t data_no_stop_words.csv   sorted_words.txt\n"]}],"source":["import torch\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import random\n","import pickle\n","import os\n","import nltk\n","from torch import nn\n","from torch.utils.data import DataLoader, TensorDataset\n","import tensorflow as tf \n","import string\n","import re\n","import pandas as pd \n","from collections import Counter\n","from torch.utils.data import DataLoader, TensorDataset\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('averaged_perceptron_tagger')\n","from nltk import word_tokenize, pos_tag\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","import nltk\n","nltk.download('omw-1.4')\n","drive.mount('/content/drive')\n","!ls '/content/drive/Shareddrives/Deep Learning/DeepLearning_2022/Final project/Data/'\n","myDrive = '/content/drive/Shareddrives/Deep Learning/DeepLearning_2022/Final project/Data/'\n","results_path = '/content/drive/Shareddrives/Deep Learning/DeepLearning_2022/Final project/Results/'"]},{"cell_type":"markdown","source":["Import data"],"metadata":{"id":"zyL4EhLc6p1_"}},{"cell_type":"code","source":["data = pd.read_csv(f'{myDrive}data_clean.csv')\n","file1 = open(f'{myDrive}all_words.txt', 'r')\n","file2 = open(f'{myDrive}sorted_words.txt', 'r')\n","all_words = file1.read().splitlines()\n","count_words = Counter(all_words)\n","sorted_words=count_words.most_common(len(all_words))\n","vocab_to_int={w:i+1 for i,(w,c) in enumerate(sorted_words)}\n"],"metadata":{"id":"WCHD8BT3NEk9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvfBrWEkg2NK"},"outputs":[],"source":["data.sentiment = [0 if each == \"negative\" else 1 for each in data.sentiment]"]},{"cell_type":"markdown","source":["Encode the words of the reviews to integers using vocab_to_int dictionary"],"metadata":{"id":"EK5GUg9e6ypE"}},{"cell_type":"code","source":["def encode_review(review):\n","  encoded_review=list()\n","  words = review.split(' ')\n","  for word in words:\n","    encoded_review.append(vocab_to_int[word])\n","\n","  return encoded_review"],"metadata":{"id":"6Ds4jKunxtMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data2 = data.copy()\n","data['review'] = data.apply(lambda row: encode_review(row['review']), axis = 1)\n","data['review_length'] = data.apply(lambda row: len(row['review']), axis = 1)"],"metadata":{"id":"fFN9zwKVxbur"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The sequence length of our sentences should be the same. Thus, we use two techniques: \n","1. Calculate the average length of reviews (in words) and truncate or pad each review to have such amount of words. \n","2. Calculate the maximum length of reviews and add padding to the rest so that all have this length."],"metadata":{"id":"4CpIR3VRClTm"}},{"cell_type":"code","source":["sequence_length = int(sum(list(data['review_length']))/data.shape[0]) # average length\n","sequence_length_max = max(list(data['review_length']))\n","print(sequence_length)\n","\n","def truncate_or_pad(review):\n","  num_words = len(review)\n","  if num_words<sequence_length:\n","    zeros = list(np.zeros(sequence_length-num_words))\n","    new = zeros+review\n","  else:\n","    new = review[:sequence_length]\n","\n","  return new\n","\n","def pad(review):\n","  num_words = len(review)\n","  new = review\n","  if num_words<sequence_length_max:\n","    zeros = list(np.zeros(sequence_length_max-num_words))\n","    new = zeros+review\n","  return new\n","\n","#test = truncate_or_pad(data.review[0])\n","data_pad = data.copy()\n","data['review'] = data.apply(lambda row: truncate_or_pad(row['review']), axis = 1)\n","data_pad['review'] = data_pad.apply(lambda row: pad(row['review']), axis = 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W6-idfop8vyj","executionInfo":{"status":"ok","timestamp":1655304464589,"user_tz":-120,"elapsed":14233,"user":{"displayName":"Klaus Ditterich Martin","userId":"12554408970924757965"}},"outputId":"f3755982-c897-4d14-9f91-91289d8375ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["118\n"]}]},{"cell_type":"markdown","source":["Store the reviews in a matrix that will be the input data of our model"],"metadata":{"id":"aSeMNJLxhPhI"}},{"cell_type":"code","source":["data_matrix = np.zeros((data.shape[0], sequence_length), dtype=int)\n","for i in range(data.shape[0]):\n","  data_matrix[i, :] = np.array(data.review[i])\n","print(data_matrix[0:15, :])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZ76eJsLA_gq","executionInfo":{"status":"ok","timestamp":1655304465768,"user_tz":-120,"elapsed":1189,"user":{"displayName":"Klaus Ditterich Martin","userId":"12554408970924757965"}},"outputId":"5fb76e6d-bddc-4e8e-d064-7b0837ded14d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[    3  1020   326 ...    23  1356    12]\n"," [    0     0     0 ...  1816  1676    19]\n"," [    0     0     0 ...    13     6   122]\n"," ...\n"," [ 3771   105   110 ...   631     2   546]\n"," [    0     0     0 ...    23    13   341]\n"," [    0     0     0 ...  1104 11136  6458]]\n","85\n"]}]},{"cell_type":"code","source":["data_matrix_pad = np.zeros((data_pad.shape[0], sequence_length_max), dtype=int)\n","for i in range(data_pad.shape[0]):\n","  data_matrix_pad[i, :] = np.array(data_pad.review[i])\n","print(data_matrix_pad[0:15, :])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1jOW5LjHw4u8","executionInfo":{"status":"ok","timestamp":1655304471873,"user_tz":-120,"elapsed":6108,"user":{"displayName":"Klaus Ditterich Martin","userId":"12554408970924757965"}},"outputId":"c8e8432b-1c27-4448-b49f-3947695d5c3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[    0     0     0 ...   469  3431   323]\n"," [    0     0     0 ...  1816  1676    19]\n"," [    0     0     0 ...    13     6   122]\n"," ...\n"," [    0     0     0 ...   179    90   160]\n"," [    0     0     0 ...    23    13   341]\n"," [    0     0     0 ...  1104 11136  6458]]\n","85\n"]}]},{"cell_type":"markdown","source":["# Create and train LSTM model "],"metadata":{"id":"AWkYsBs8Gc9z"}},{"cell_type":"markdown","source":["Now, split into train and test datasets"],"metadata":{"id":"lLwr6-PDhXC8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"epUZVsoEeImz"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(data_matrix_pad, list(data_pad.sentiment), test_size=0.3)"]},{"cell_type":"code","source":["train_data=TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n","test_data=TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))"],"metadata":{"id":"xTR1CMCrGiXi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Divide the data into batches"],"metadata":{"id":"3v29vUS0haDU"}},{"cell_type":"code","source":["batch_size=50\n","train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"lGngfttiHFSp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's define our LSTM neural network"],"metadata":{"id":"Gw7dBN90hdAk"}},{"cell_type":"code","source":["class SentimentalLSTM(nn.Module):\n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):    \n","        super().__init__()\n","        self.output_size=output_size\n","        self.n_layers=n_layers\n","        self.hidden_dim=hidden_dim\n","        \n","        #Embedding and LSTM layers\n","        self.embedding=nn.Embedding(vocab_size, embedding_dim)\n","\n","        self.lstm=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n","        \n","        #dropout layer\n","        self.dropout=nn.Dropout(0.5)\n","        \n","        #Linear and sigmoid layer\n","        self.fc1=nn.Linear(hidden_dim, 64)\n","        self.fc2=nn.Linear(64, 16)\n","        self.fc3=nn.Linear(16,output_size)\n","        self.sigmoid=nn.Sigmoid()\n","        \n","    def forward(self, x, hidden):\n","        \"\"\"\n","        Perform a forward pass of our model on some input and hidden state.\n","        \"\"\"\n","        batch_size=x.size()\n","        \n","        #Embadding and LSTM output\n","        embedd=self.embedding(x)\n","        lstm_out, hidden=self.lstm(embedd, hidden)\n","        \n","        #stack up the lstm output\n","        lstm_out=lstm_out.contiguous().view(-1, self.hidden_dim)\n","        \n","        #dropout and fully connected layers\n","        out=self.dropout(lstm_out)\n","        out=self.fc1(out)\n","        out=self.dropout(out)\n","        out=self.fc2(out)\n","        out=self.dropout(out)\n","        out=self.fc3(out)\n","        sig_out=self.sigmoid(out)\n","        \n","        sig_out=sig_out.view(batch_size, -1)\n","        sig_out=sig_out[:, -1]\n","        \n","        return sig_out, hidden\n","    \n","    def init_hidden(self, batch_size):\n","        \"\"\"Initialize Hidden STATE\"\"\"\n","        # Create two new sensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        \n","        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","        return hidden"],"metadata":{"id":"JD2nACGXowEK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following function will be used to train the model using train dataset"],"metadata":{"id":"nm_xTwiMhrye"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pny7945jbdtW"},"outputs":[],"source":["def train_loop(model, model_name = 'model.ckpt', device = 'cuda'):\n","    model.train()\n","    total_step = len(train_loader)\n","    losses_list = []\n","    criterion = nn.BCELoss()\n","\n","    for epoch in range(num_epochs):\n","        loss_avg = 0\n","        h = net.init_hidden(batch_size)\n","        nBatches = 0\n","        # TRAINING LOOP\n","        for i, (review, sentiment) in enumerate(train_loader):\n","            review = review.type(torch.LongTensor).to(device)\n","            sentiment = sentiment.to(device)\n","\n","            h = tuple([each.data for each in h])\n","            net.zero_grad()\n","\n","            # Forward pass\n","            outputs,h = model(review, h)\n","            # Calculate CE_loss\n","            loss = criterion(outputs, sentiment)\n","\n","            # Backpropagate\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            loss_avg += loss.cpu().item()\n","            nBatches+=1\n","            if (i+1) % 200 == 0:\n","                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, loss_avg / nBatches))\n","        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                       .format(epoch+1, num_epochs, i+1, total_step, loss_avg / nBatches))\n","        losses_list.append(loss_avg / nBatches)\n","        torch.save(model.state_dict(), results_path+model_name)\n"]},{"cell_type":"markdown","source":["Test function to use after train, with the test dataset"],"metadata":{"id":"m8H5yhs4huzy"}},{"cell_type":"code","source":["#Load and test function\n","def load_and_test(model, device='cuda', model_name = 'model.ckpt'):\n","  model.load_state_dict(torch.load(results_path+model_name))\n","  criterion = nn.BCELoss()\n","  h = net.init_hidden(batch_size)\n","  # Test the model\n","  model.eval() # Set the model in evaluation mode\n","\n","  # Compute testing accuracy\n","  with torch.no_grad():\n","      correct = 0\n","      total = 0\n","      for review, sentiment in test_loader:\n","          h = tuple([each.data for each in h])\n","\n","          review = review.type(torch.LongTensor).to(device)\n","          sentiment = sentiment.to(device)\n","          # get network predictions\n","          outputs, h = model(review, h)\n","\n","          # get predicted class\n","          predicted = torch.round(outputs)\n","          # compare with the ground-truth\n","          total += sentiment.size(0)\n","          correct += (predicted == sentiment).sum().item()\n","\n","      print('Test Accuracy of the model: {} %'.format(100 * correct / total))"],"metadata":{"id":"ECBEhZiXX2pd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instantiation of the model with hyperparams:"],"metadata":{"id":"bd9F2-oihzAP"}},{"cell_type":"code","source":["vocab_size = 101145\n","output_size = 1\n","embedding_dim = 100\n","hidden_dim = 256\n","n_layers = 2"],"metadata":{"id":"n7jXB-7Gm_St"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Checking different values for Adam SGD optimizer"],"metadata":{"id":"DLpkugTnh4gX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AECrU3kabgbE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ea70805-c1ca-4189-e747-5df5ce0952d8","executionInfo":{"status":"ok","timestamp":1655307914788,"user_tz":-120,"elapsed":2772779,"user":{"displayName":"Klaus Ditterich Martin","userId":"12554408970924757965"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/6], Step [200/700], Loss: 0.6488\n","Epoch [1/6], Step [400/700], Loss: 0.6151\n","Epoch [1/6], Step [600/700], Loss: 0.6418\n","Epoch [1/6], Step [700/700], Loss: 0.6492\n","Epoch [2/6], Step [200/700], Loss: 0.6860\n","Epoch [2/6], Step [400/700], Loss: 0.6406\n","Epoch [2/6], Step [600/700], Loss: 0.5756\n","Epoch [2/6], Step [700/700], Loss: 0.5499\n","Epoch [3/6], Step [200/700], Loss: 0.3475\n","Epoch [3/6], Step [400/700], Loss: 0.3327\n","Epoch [3/6], Step [600/700], Loss: 0.3270\n","Epoch [3/6], Step [700/700], Loss: 0.3232\n","Epoch [4/6], Step [200/700], Loss: 0.2342\n","Epoch [4/6], Step [400/700], Loss: 0.2345\n","Epoch [4/6], Step [600/700], Loss: 0.2317\n","Epoch [4/6], Step [700/700], Loss: 0.2308\n","Epoch [5/6], Step [200/700], Loss: 0.1700\n","Epoch [5/6], Step [400/700], Loss: 0.1728\n","Epoch [5/6], Step [600/700], Loss: 0.1725\n","Epoch [5/6], Step [700/700], Loss: 0.1739\n","Epoch [6/6], Step [200/700], Loss: 0.1287\n","Epoch [6/6], Step [400/700], Loss: 0.1236\n","Epoch [6/6], Step [600/700], Loss: 0.1272\n","Epoch [6/6], Step [700/700], Loss: 0.1285\n","Test Accuracy of the model: 87.94 %\n","Epoch [1/6], Step [200/700], Loss: 0.6631\n","Epoch [1/6], Step [400/700], Loss: 0.6108\n","Epoch [1/6], Step [600/700], Loss: 0.5737\n","Epoch [1/6], Step [700/700], Loss: 0.5664\n","Epoch [2/6], Step [200/700], Loss: 0.4614\n","Epoch [2/6], Step [400/700], Loss: 0.4301\n","Epoch [2/6], Step [600/700], Loss: 0.4163\n","Epoch [2/6], Step [700/700], Loss: 0.4159\n","Epoch [3/6], Step [200/700], Loss: 0.3787\n","Epoch [3/6], Step [400/700], Loss: 0.3622\n","Epoch [3/6], Step [600/700], Loss: 0.3552\n","Epoch [3/6], Step [700/700], Loss: 0.3545\n","Epoch [4/6], Step [200/700], Loss: 0.3061\n","Epoch [4/6], Step [400/700], Loss: 0.2972\n","Epoch [4/6], Step [600/700], Loss: 0.3027\n","Epoch [4/6], Step [700/700], Loss: 0.3016\n","Epoch [5/6], Step [200/700], Loss: 0.2464\n","Epoch [5/6], Step [400/700], Loss: 0.2924\n","Epoch [5/6], Step [600/700], Loss: 0.2977\n","Epoch [5/6], Step [700/700], Loss: 0.2949\n","Epoch [6/6], Step [200/700], Loss: 0.2446\n","Epoch [6/6], Step [400/700], Loss: 0.2502\n","Epoch [6/6], Step [600/700], Loss: 0.2474\n","Epoch [6/6], Step [700/700], Loss: 0.2598\n","Test Accuracy of the model: 85.02 %\n","Epoch [1/6], Step [200/700], Loss: 0.6410\n","Epoch [1/6], Step [400/700], Loss: 0.6493\n","Epoch [1/6], Step [600/700], Loss: 0.6644\n","Epoch [1/6], Step [700/700], Loss: 0.6684\n","Epoch [2/6], Step [200/700], Loss: 0.6881\n","Epoch [2/6], Step [400/700], Loss: 0.6406\n","Epoch [2/6], Step [600/700], Loss: 0.5844\n","Epoch [2/6], Step [700/700], Loss: 0.5571\n","Epoch [3/6], Step [200/700], Loss: 0.3215\n","Epoch [3/6], Step [400/700], Loss: 0.3157\n","Epoch [3/6], Step [600/700], Loss: 0.3107\n","Epoch [3/6], Step [700/700], Loss: 0.3066\n","Epoch [4/6], Step [200/700], Loss: 0.1947\n","Epoch [4/6], Step [400/700], Loss: 0.2008\n","Epoch [4/6], Step [600/700], Loss: 0.1977\n","Epoch [4/6], Step [700/700], Loss: 0.1982\n","Epoch [5/6], Step [200/700], Loss: 0.1292\n","Epoch [5/6], Step [400/700], Loss: 0.1302\n","Epoch [5/6], Step [600/700], Loss: 0.1336\n","Epoch [5/6], Step [700/700], Loss: 0.1338\n","Epoch [6/6], Step [200/700], Loss: 0.0804\n","Epoch [6/6], Step [400/700], Loss: 0.0825\n","Epoch [6/6], Step [600/700], Loss: 0.0877\n","Epoch [6/6], Step [700/700], Loss: 0.0890\n","Test Accuracy of the model: 87.26 %\n"]}],"source":["num_epochs = 6\n","for lr_i in [0.001, 0.0005, 0.002]:\n","  net = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","  net = net.cuda()\n","  optimizer = torch.optim.Adam(net.parameters(),lr = lr_i)\n","\n","  # Device configuration (choose GPU if it is available )\n","  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","  train_loop(net, model_name = 'model_4.ckpt')\n","  load_and_test(net,  model_name = 'model_4.ckpt')"]},{"cell_type":"markdown","source":["Checking different values for momentum SGD optimizer"],"metadata":{"id":"EQPgZzRxiONn"}},{"cell_type":"code","source":["for lr_i in [0.001, 0.01, 0.05, 0.06]:\n","  net = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","  net = net.cuda()\n","  optimizer = torch.optim.SGD(net.parameters(), lr=lr_i, momentum=0.9)\n","\n","  # Device configuration (choose GPU if it is available )\n","  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","  train_loop(net, model_name = 'model_4.ckpt')\n","  load_and_test(net,  model_name = 'model_4.ckpt')"],"metadata":{"id":"5DaKKJRt0peq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655311659396,"user_tz":-120,"elapsed":3488828,"user":{"displayName":"Klaus Ditterich Martin","userId":"12554408970924757965"}},"outputId":"74a963d0-7736-4033-f014-35d7fb2834d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/6], Step [200/700], Loss: 0.6952\n","Epoch [1/6], Step [400/700], Loss: 0.6945\n","Epoch [1/6], Step [600/700], Loss: 0.6943\n","Epoch [1/6], Step [700/700], Loss: 0.6942\n","Epoch [2/6], Step [200/700], Loss: 0.6932\n","Epoch [2/6], Step [400/700], Loss: 0.6933\n","Epoch [2/6], Step [600/700], Loss: 0.6934\n","Epoch [2/6], Step [700/700], Loss: 0.6934\n","Epoch [3/6], Step [200/700], Loss: 0.6933\n","Epoch [3/6], Step [400/700], Loss: 0.6934\n","Epoch [3/6], Step [600/700], Loss: 0.6933\n","Epoch [3/6], Step [700/700], Loss: 0.6932\n","Epoch [4/6], Step [200/700], Loss: 0.6934\n","Epoch [4/6], Step [400/700], Loss: 0.6933\n","Epoch [4/6], Step [600/700], Loss: 0.6933\n","Epoch [4/6], Step [700/700], Loss: 0.6934\n","Epoch [5/6], Step [200/700], Loss: 0.6931\n","Epoch [5/6], Step [400/700], Loss: 0.6932\n","Epoch [5/6], Step [600/700], Loss: 0.6933\n","Epoch [5/6], Step [700/700], Loss: 0.6933\n","Epoch [6/6], Step [200/700], Loss: 0.6931\n","Epoch [6/6], Step [400/700], Loss: 0.6932\n","Epoch [6/6], Step [600/700], Loss: 0.6931\n","Epoch [6/6], Step [700/700], Loss: 0.6931\n","Test Accuracy of the model: 49.58 %\n","Epoch [1/6], Step [200/700], Loss: 0.6938\n","Epoch [1/6], Step [400/700], Loss: 0.6936\n","Epoch [1/6], Step [600/700], Loss: 0.6936\n","Epoch [1/6], Step [700/700], Loss: 0.6935\n","Epoch [2/6], Step [200/700], Loss: 0.6933\n","Epoch [2/6], Step [400/700], Loss: 0.6930\n","Epoch [2/6], Step [600/700], Loss: 0.6928\n","Epoch [2/6], Step [700/700], Loss: 0.6925\n","Epoch [3/6], Step [200/700], Loss: 0.6909\n","Epoch [3/6], Step [400/700], Loss: 0.6895\n","Epoch [3/6], Step [600/700], Loss: 0.6875\n","Epoch [3/6], Step [700/700], Loss: 0.6860\n","Epoch [4/6], Step [200/700], Loss: 0.6673\n","Epoch [4/6], Step [400/700], Loss: 0.6640\n","Epoch [4/6], Step [600/700], Loss: 0.6605\n","Epoch [4/6], Step [700/700], Loss: 0.6566\n","Epoch [5/6], Step [200/700], Loss: 0.6170\n","Epoch [5/6], Step [400/700], Loss: 0.6130\n","Epoch [5/6], Step [600/700], Loss: 0.6002\n","Epoch [5/6], Step [700/700], Loss: 0.5955\n","Epoch [6/6], Step [200/700], Loss: 0.5656\n","Epoch [6/6], Step [400/700], Loss: 0.5424\n","Epoch [6/6], Step [600/700], Loss: 0.5355\n","Epoch [6/6], Step [700/700], Loss: 0.5288\n","Test Accuracy of the model: 75.27333333333333 %\n","Epoch [1/6], Step [200/700], Loss: 0.6938\n","Epoch [1/6], Step [400/700], Loss: 0.6915\n","Epoch [1/6], Step [600/700], Loss: 0.6820\n","Epoch [1/6], Step [700/700], Loss: 0.6753\n","Epoch [2/6], Step [200/700], Loss: 0.6315\n","Epoch [2/6], Step [400/700], Loss: 0.6450\n","Epoch [2/6], Step [600/700], Loss: 0.6472\n","Epoch [2/6], Step [700/700], Loss: 0.6431\n","Epoch [3/6], Step [200/700], Loss: 0.6979\n","Epoch [3/6], Step [400/700], Loss: 0.6851\n","Epoch [3/6], Step [600/700], Loss: 0.6636\n","Epoch [3/6], Step [700/700], Loss: 0.6546\n","Epoch [4/6], Step [200/700], Loss: 0.6461\n","Epoch [4/6], Step [400/700], Loss: 0.6123\n","Epoch [4/6], Step [600/700], Loss: 0.5831\n","Epoch [4/6], Step [700/700], Loss: 0.5686\n","Epoch [5/6], Step [200/700], Loss: 0.4527\n","Epoch [5/6], Step [400/700], Loss: 0.4456\n","Epoch [5/6], Step [600/700], Loss: 0.4376\n","Epoch [5/6], Step [700/700], Loss: 0.4343\n","Epoch [6/6], Step [200/700], Loss: 0.3983\n","Epoch [6/6], Step [400/700], Loss: 0.3927\n","Epoch [6/6], Step [600/700], Loss: 0.3852\n","Epoch [6/6], Step [700/700], Loss: 0.3835\n","Test Accuracy of the model: 83.18666666666667 %\n","Epoch [1/6], Step [200/700], Loss: 0.6945\n","Epoch [1/6], Step [400/700], Loss: 0.6935\n","Epoch [1/6], Step [600/700], Loss: 0.6848\n","Epoch [1/6], Step [700/700], Loss: 0.6794\n","Epoch [2/6], Step [200/700], Loss: 0.6388\n","Epoch [2/6], Step [400/700], Loss: 0.6319\n","Epoch [2/6], Step [600/700], Loss: 0.6191\n","Epoch [2/6], Step [700/700], Loss: 0.6202\n","Epoch [3/6], Step [200/700], Loss: 0.5597\n","Epoch [3/6], Step [400/700], Loss: 0.5711\n","Epoch [3/6], Step [600/700], Loss: 0.5882\n","Epoch [3/6], Step [700/700], Loss: 0.5844\n","Epoch [4/6], Step [200/700], Loss: 0.5247\n","Epoch [4/6], Step [400/700], Loss: 0.5315\n","Epoch [4/6], Step [600/700], Loss: 0.5141\n","Epoch [4/6], Step [700/700], Loss: 0.5108\n","Epoch [5/6], Step [200/700], Loss: 0.4399\n","Epoch [5/6], Step [400/700], Loss: 0.4333\n","Epoch [5/6], Step [600/700], Loss: 0.4235\n","Epoch [5/6], Step [700/700], Loss: 0.4185\n","Epoch [6/6], Step [200/700], Loss: 0.3801\n","Epoch [6/6], Step [400/700], Loss: 0.3775\n","Epoch [6/6], Step [600/700], Loss: 0.3752\n","Epoch [6/6], Step [700/700], Loss: 0.3821\n","Test Accuracy of the model: 82.63333333333334 %\n"]}]},{"cell_type":"code","source":["for mom in [0.8, 0.85, 0.89, 0.95]:\n","  net = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","  net = net.cuda()\n","  optimizer = torch.optim.SGD(net.parameters(), lr=0.05, momentum=mom)\n","\n","  # Device configuration (choose GPU if it is available )\n","  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","  train_loop(net, model_name = 'model_4.ckpt')\n","  load_and_test(net,  model_name = 'model_4.ckpt')"],"metadata":{"id":"YIdEWywu0-e2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655315223192,"user_tz":-120,"elapsed":3481682,"user":{"displayName":"Klaus Ditterich Martin","userId":"12554408970924757965"}},"outputId":"2dc0dcd9-8255-4556-8384-b6b03a38d545"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/6], Step [200/700], Loss: 0.6941\n","Epoch [1/6], Step [400/700], Loss: 0.6932\n","Epoch [1/6], Step [600/700], Loss: 0.6929\n","Epoch [1/6], Step [700/700], Loss: 0.6919\n","Epoch [2/6], Step [200/700], Loss: 0.6705\n","Epoch [2/6], Step [400/700], Loss: 0.6590\n","Epoch [2/6], Step [600/700], Loss: 0.6531\n","Epoch [2/6], Step [700/700], Loss: 0.6523\n","Epoch [3/6], Step [200/700], Loss: 0.6161\n","Epoch [3/6], Step [400/700], Loss: 0.6089\n","Epoch [3/6], Step [600/700], Loss: 0.6089\n","Epoch [3/6], Step [700/700], Loss: 0.6059\n","Epoch [4/6], Step [200/700], Loss: 0.5675\n","Epoch [4/6], Step [400/700], Loss: 0.5702\n","Epoch [4/6], Step [600/700], Loss: 0.5763\n","Epoch [4/6], Step [700/700], Loss: 0.5769\n","Epoch [5/6], Step [200/700], Loss: 0.6953\n","Epoch [5/6], Step [400/700], Loss: 0.6944\n","Epoch [5/6], Step [600/700], Loss: 0.6941\n","Epoch [5/6], Step [700/700], Loss: 0.6940\n","Epoch [6/6], Step [200/700], Loss: 0.6937\n","Epoch [6/6], Step [400/700], Loss: 0.6933\n","Epoch [6/6], Step [600/700], Loss: 0.6927\n","Epoch [6/6], Step [700/700], Loss: 0.6918\n","Test Accuracy of the model: 58.153333333333336 %\n","Epoch [1/6], Step [200/700], Loss: 0.6937\n","Epoch [1/6], Step [400/700], Loss: 0.6934\n","Epoch [1/6], Step [600/700], Loss: 0.6923\n","Epoch [1/6], Step [700/700], Loss: 0.6901\n","Epoch [2/6], Step [200/700], Loss: 0.6575\n","Epoch [2/6], Step [400/700], Loss: 0.6469\n","Epoch [2/6], Step [600/700], Loss: 0.6371\n","Epoch [2/6], Step [700/700], Loss: 0.6303\n","Epoch [3/6], Step [200/700], Loss: 0.5630\n","Epoch [3/6], Step [400/700], Loss: 0.5450\n","Epoch [3/6], Step [600/700], Loss: 0.5409\n","Epoch [3/6], Step [700/700], Loss: 0.5388\n","Epoch [4/6], Step [200/700], Loss: 0.5060\n","Epoch [4/6], Step [400/700], Loss: 0.4846\n","Epoch [4/6], Step [600/700], Loss: 0.4827\n","Epoch [4/6], Step [700/700], Loss: 0.4807\n","Epoch [5/6], Step [200/700], Loss: 0.4450\n","Epoch [5/6], Step [400/700], Loss: 0.4611\n","Epoch [5/6], Step [600/700], Loss: 0.4518\n","Epoch [5/6], Step [700/700], Loss: 0.4497\n","Epoch [6/6], Step [200/700], Loss: 0.4325\n","Epoch [6/6], Step [400/700], Loss: 0.4361\n","Epoch [6/6], Step [600/700], Loss: 0.5038\n","Epoch [6/6], Step [700/700], Loss: 0.5285\n","Test Accuracy of the model: 62.39333333333333 %\n","Epoch [1/6], Step [200/700], Loss: 0.6942\n","Epoch [1/6], Step [400/700], Loss: 0.6936\n","Epoch [1/6], Step [600/700], Loss: 0.6909\n","Epoch [1/6], Step [700/700], Loss: 0.6876\n","Epoch [2/6], Step [200/700], Loss: 0.6511\n","Epoch [2/6], Step [400/700], Loss: 0.6427\n","Epoch [2/6], Step [600/700], Loss: 0.6291\n","Epoch [2/6], Step [700/700], Loss: 0.6201\n","Epoch [3/6], Step [200/700], Loss: 0.5336\n","Epoch [3/6], Step [400/700], Loss: 0.5281\n","Epoch [3/6], Step [600/700], Loss: 0.5239\n","Epoch [3/6], Step [700/700], Loss: 0.5266\n","Epoch [4/6], Step [200/700], Loss: 0.5961\n","Epoch [4/6], Step [400/700], Loss: 0.5586\n","Epoch [4/6], Step [600/700], Loss: 0.5333\n","Epoch [4/6], Step [700/700], Loss: 0.5200\n","Epoch [5/6], Step [200/700], Loss: 0.4068\n","Epoch [5/6], Step [400/700], Loss: 0.4123\n","Epoch [5/6], Step [600/700], Loss: 0.4095\n","Epoch [5/6], Step [700/700], Loss: 0.4069\n","Epoch [6/6], Step [200/700], Loss: 0.3628\n","Epoch [6/6], Step [400/700], Loss: 0.3656\n","Epoch [6/6], Step [600/700], Loss: 0.3633\n","Epoch [6/6], Step [700/700], Loss: 0.3608\n","Test Accuracy of the model: 83.40666666666667 %\n","Epoch [1/6], Step [200/700], Loss: 0.6939\n","Epoch [1/6], Step [400/700], Loss: 0.6937\n","Epoch [1/6], Step [600/700], Loss: 0.6841\n","Epoch [1/6], Step [700/700], Loss: 0.6815\n","Epoch [2/6], Step [200/700], Loss: 0.6421\n","Epoch [2/6], Step [400/700], Loss: 0.6349\n","Epoch [2/6], Step [600/700], Loss: 0.6217\n","Epoch [2/6], Step [700/700], Loss: 0.6149\n","Epoch [3/6], Step [200/700], Loss: 0.5117\n","Epoch [3/6], Step [400/700], Loss: 0.5344\n","Epoch [3/6], Step [600/700], Loss: 0.5361\n","Epoch [3/6], Step [700/700], Loss: 0.5322\n","Epoch [4/6], Step [200/700], Loss: 0.4381\n","Epoch [4/6], Step [400/700], Loss: 0.4395\n","Epoch [4/6], Step [600/700], Loss: 0.4316\n","Epoch [4/6], Step [700/700], Loss: 0.4330\n","Epoch [5/6], Step [200/700], Loss: 0.3884\n","Epoch [5/6], Step [400/700], Loss: 0.3946\n","Epoch [5/6], Step [600/700], Loss: 0.3841\n","Epoch [5/6], Step [700/700], Loss: 0.3820\n","Epoch [6/6], Step [200/700], Loss: 0.3518\n","Epoch [6/6], Step [400/700], Loss: 0.3476\n","Epoch [6/6], Step [600/700], Loss: 0.3442\n","Epoch [6/6], Step [700/700], Loss: 0.3445\n","Test Accuracy of the model: 84.38666666666667 %\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Zeros_FinalLab.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}